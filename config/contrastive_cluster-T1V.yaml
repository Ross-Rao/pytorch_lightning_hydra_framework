# config.yaml

defaults:
    # hydra settings
    - hydra  # refer to `hydra.yaml`
    # dataset settings
    - dataset: HCC-MVI-ROI64-T1-no-patch-stack-no-recon-5  # refer to `dataset/HCC-WCH.yaml`
    - _self_  # 占位符：表示当前文件，在defaults的最后一行声明，可以保证当前文件的配置可覆盖之前的配置

# trainer settings
stage_change_epoch: 100  # stage change epoch
anchor_update_frequency: 10  # anchor update frequency
max_search_ratio: 1.0  # max search ratio
expand_ratio: 30
#  contains the configuration for the pytorch_lightning.trainer.
callbacks:  # inline definition of callbacks, don't change the name
    # relevant input arguments in main.py
    CustomModelCheckpoint:
        filename: "model_{epoch:02d}_{val_loss:.2f}"
        monitor: "val_loss"  # log in module/example_module.py
        save_top_k: 7
        mode: "min"
        start_epoch: '${stage_change_epoch}'
        save_last: True
    CustomEarlyStopping:
        monitor: 'val_loss'
        patience: 20
        start_epoch: '${stage_change_epoch}'
        mode: 'min'
        check_on_train_epoch_end: False
    TQDMProgressBar:
        refresh_rate: 1
    LearningRateLogger:
trainer:  # inline definition of trainer, don't change the name
    max_epochs: 200
    accelerator: 'gpu'
    devices: 1
    check_val_every_n_epoch: 2
    log_every_n_steps: 2

# module settings: used in module/example_module.py
criterion:
    criterion:
        - 'Stage1ModelLoss'
        - 'Stage2ModelLoss'
    criterion_params:
        - 
        - num_classes: '${model.model_params[1].num_classes}'
          cls_class_ratio: 0.3

optimizer:
    optimizer:
        - Adam
        - Adam
    optimizer_params:
        - lr: 2e-4
          betas: [0.9, 0.999]
        - lr: 5e-5
          betas: [0.9, 0.999]
n_cluster: 6
model:
    model:
        - 'Stage1Model'
        - 'Stage2Model'
    model_params:
        - n_cluster: '${n_cluster}'
          perceptual_loss_ratio: 1.2
          neighbors: 4
        - num_classes: 2
          n_cluster: '${n_cluster}'
          seq_len: 3
          softmax_temp: 0.4
          dropout: 0.5  # 0.1
          attention_dropout: 0


lr_scheduler:
    lr_scheduler: 'ReduceLROnPlateau'
    lr_scheduler_params:
        mode: "min"
        factor: 0.8
        patience: 8
        min_lr: 1e-6
    lr_scheduler_other_params:
        monitor: "val_loss"
        interval: "epoch"
        frequency: 2