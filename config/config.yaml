# config.yaml

defaults:
    # hydra settings
    - hydra  # refer to `hydra.yaml`
    # dataset settings
    - dataset:   # refer to `dataset/{---}.yaml`
    - _self_  # 占位符：表示当前文件，在defaults的最后一行声明，可以保证当前文件的配置可覆盖之前的配置

# trainer settings
#  contains the configuration for the pytorch_lightning.trainer.
callbacks:  # inline definition of callbacks, don't change the name
    # relevant input arguments in main.py
    ModelCheckpoint:
        filename: "model_{epoch:02d}_{val_loss:.2f}"
        monitor: "val_loss"  # log in module/example_module.py
        save_top_k: 3
        mode: "min"
        save_last: True
    EarlyStopping:
        monitor: 'val_loss'
        patience: 20
        mode: 'min'
    TQDMProgressBar:
        refresh_rate: 1
trainer:  # inline definition of trainer, don't change the name
    max_epochs:
    accelerator: 'gpu'
    devices: 1
    check_val_every_n_epoch: 2
    log_every_n_steps: 2

# module settings: used in module/example_module.py
criterion:
    criterion: ''
    criterion_params:
optimizer:
    optimizer: Adam
    optimizer_params:
        lr: 1e-3
        betas: [0.9, 0.999]
model:
    model: ""
    model_params:


#lr_scheduler:
#    lr_scheduler: 'ReduceLROnPlateau'
#    lr_scheduler_params:
#        mode: "min"
#        factor: 0.5
#        patience: 10
#        min_lr: 1e-6
#    lr_scheduler_other_params:
#        monitor: "Error/Val_total"
#        interval: "epoch"
#        frequency: 1