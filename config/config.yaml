# config.yaml

defaults:
    # hydra settings
    - hydra  # refer to `hydra.yaml`
    # dataset settings
    - dataset: HCC-MVI-ROI64-no-patch-no-recon  # refer to `dataset/HCC-WCH.yaml`
    - _self_  # 占位符：表示当前文件，在defaults的最后一行声明，可以保证当前文件的配置可覆盖之前的配置

# trainer settings
#  contains the configuration for the pytorch_lightning.trainer.
callbacks:  # inline definition of callbacks, don't change the name
    # relevant input arguments in main.py
    ModelCheckpoint:
        filename: "model_{epoch:02d}_{val_loss:.2f}"
        monitor: "val/loss"  # log in module/example_module.py
        save_top_k: 3
        mode: "min"
        save_last: True
    EarlyStopping:
        monitor: 'val/loss'
        patience: 20
        mode: 'min'
    TQDMProgressBar:
        refresh_rate: 1
trainer:  # inline definition of trainer, don't change the name
    max_epochs: 400
    accelerator: 'gpu'
    devices: 1
    check_val_every_n_epoch: 2
    log_every_n_steps: 2

# module settings: used in module/example_module.py
criterion:
    criterion: 'ModelLoss'
    criterion_params:
        # the memory bank should only store train samples
        # indices are reset by order: train, val, test
        # so if input index < model_param.n_samples, it should be train samples
        # if input index > n_samples, it should be val or internal test samples
        # if input index is None, it should be external test samples
        # it works by:
        # 1. `split_dataset_folds` by param `reset_split_index` in modules/monai_data_module.py
        # 2. `IndexTransformd` in utils/custom_transforms.py
        stage: 1
        n_samples: 1746  # train samples 588
        npc_dim: 128
        neighbors: 1
        temperature: 0.07
        momentum: 0.5
        const: 0
optimizer:
    optimizer: Adam
    optimizer_params:
        lr: 1e-3
        betas: [0.9, 0.999]
model:
    model: "EncoderDecoderModel"
    model_params:
        stage: 1
        n_classes: 3
        encoder_in_channels: 1
        decoder_out_channels: 1
        decoder_layers: 5
        pretrained: True
        encoder: 'resnet34'
        hidden_dim: 512
        npc_dim: 128
        activation: 'ReLU'


#lr_scheduler:
#    lr_scheduler: 'ReduceLROnPlateau'
#    lr_scheduler_params:
#        mode: "min"
#        factor: 0.5
#        patience: 10
#        min_lr: 1e-6
#    lr_scheduler_other_params:
#        monitor: "Error/Val_total"
#        interval: "epoch"
#        frequency: 1