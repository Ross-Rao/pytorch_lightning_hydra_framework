# config.yaml

defaults:
    # hydra settings
    - hydra  # refer to `hydra.yaml`
    # dataset settings
    - dataset: HCC-MVI-ROI64-DWI-no-patch-stack-no-recon  # refer to `dataset/HCC-WCH.yaml`
    - _self_  # 占位符：表示当前文件，在defaults的最后一行声明，可以保证当前文件的配置可覆盖之前的配置

# trainer settings
#  contains the configuration for the pytorch_lightning.trainer.
callbacks:  # inline definition of callbacks, don't change the name
    # relevant input arguments in main.py
    ModelCheckpoint:
        filename: "model_{epoch:02d}_{val_loss:.2f}"
        monitor: "val_loss"  # log in module/example_module.py
        save_top_k: 3
        mode: "min"
        save_last: True
    EarlyStopping:
        monitor: 'val_loss'
        patience: 20
        mode: 'min'
    TQDMProgressBar:
        refresh_rate: 1
trainer:  # inline definition of trainer, don't change the name
    max_epochs: 300
    accelerator: 'gpu'
    devices: 1
    check_val_every_n_epoch: 2
    log_every_n_steps: 2

# module settings: used in module/example_module.py
criterion:
    criterion: 'CrossEntropyLoss'
    criterion_params:
optimizer:
    optimizer: Adam
    optimizer_params:  # 64, 0.0001/ 64 0.00005
        lr: 1e-3
        betas: [0.9, 0.999]
model:
    model: "ViTModule"
    model_params:
        in_channels: 3
        num_classes: 3
        patch_size: 64
        emb_size: 256
        img_size: 64


lr_scheduler:
    lr_scheduler: 'ReduceLROnPlateau'
    lr_scheduler_params:
        mode: "min"
        factor: 0.5
        patience: 10
        min_lr: 1e-6
    lr_scheduler_other_params:
        monitor: "train/loss"
        interval: "epoch"
        frequency: 1